import streamlit as st
import pandas as pd
import openai
import os
import numpy as np
import matplotlib.pyplot as plt
import random
from wordcloud import WordCloud
import base64
from datetime import datetime
import re
import json

# For better Arabic font handling in Matplotlib:
import matplotlib
matplotlib.rcParams['font.family'] = 'DejaVu Sans'
matplotlib.rcParams['font.size'] = 14
matplotlib.rcParams['axes.unicode_minus'] = False

# Import your auth system
from auth_system import require_auth, verify_jwt_token


##############################################################################
# 0) UI TEXT DICTIONARIES FOR INTERFACE ELEMENTS
##############################################################################
# This allows us to translate basic interface labels into Arabic, French, English, Darija.
ui_texts = {
    "English": {
        "title": "ğŸ“Š User Dashboard",
        "header_comments": "ğŸ’¬ Citizen Comments (REMACTO)",
        "label_normalize": "ğŸ§¹ Apply Basic Arabic Normalization (Optional)",
        "analysis_section": "ğŸ§  GPT-Based Sentiment & Summaries + Polarity",
        "proposal_header": "ğŸ“ Submit a New Proposal or Feedback",
        "proposal_title_label": "ğŸ“Œ Proposal Title",
        "proposal_description_label": "ğŸ§¾ Proposal Description",
        "proposal_submit_button": "ğŸ“¤ Submit Proposal",
        "feedback_label": "ğŸ’­ Your Feedback",
        "feedback_button": "ğŸ“¬ Send Feedback",
        "logout_button": "ğŸ”“ Logout",
        "no_comments_msg": "âš ï¸ No REMACTO Comments available.",
        "original_data_label": "ğŸ“‹ Original Data (first 10 rows):",
        "norm_success": "âœ… Text normalization applied.",
        "no_token_msg": "âš ï¸ No token found in session. Please go back and log in.",
        "token_invalid": "âŒ Token is invalid or expired. Please log in again.",
        "logged_in_as": "âœ… You are logged in as:",
        "role_label": "(Role: ",
        "closing_paren": ")",
        "projects_header": "ğŸ—ï¸ Municipal Projects (REMACTO)",
        "no_projects_msg": "âš ï¸ No REMACTO Projects available.",
        "projects_data_preview": "ğŸ“‚ Projects Data (Preview)",
        "summaries_of_themes": "ğŸ“ Summaries of Project Themes",
        "proposals_feedback_tab": "ğŸ—³ï¸ Submit a New Proposal or Feedback",
        "extra_visualizations_tab": "ğŸ“ˆ Extra Visualizations & Analysis",
        "all_user_inputs_tab": "ğŸ—ƒï¸ All Stored Inputs from Citizens"
    },
    "Arabic": {
        "title": "ğŸ“Š Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…",
        "header_comments": "ğŸ’¬ ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ù…ÙˆØ§Ø·Ù†ÙŠÙ† (Ø±ÙŠÙ…Ø§ÙƒØªÙˆ)",
        "label_normalize": "ğŸ§¹ ØªØ·Ø¨ÙŠÙ‚ ØªÙ†Ù‚ÙŠØ­ Ø¨Ø³ÙŠØ· Ù„Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)",
        "analysis_section": "ğŸ§  ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… GPT",
        "proposal_header": "ğŸ“ Ø¥Ø¶Ø§ÙØ© Ø§Ù‚ØªØ±Ø§Ø­ Ø¬Ø¯ÙŠØ¯ Ø£Ùˆ Ù…Ù„Ø§Ø­Ø¸Ø§Øª",
        "proposal_title_label": "ğŸ“Œ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­",
        "proposal_description_label": "ğŸ§¾ ÙˆØµÙ Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­",
        "proposal_submit_button": "ğŸ“¤ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­",
        "feedback_label": "ğŸ’­ Ù…Ù„Ø§Ø­Ø¸Ø§ØªÙƒ",
        "feedback_button": "ğŸ“¬ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª",
        "logout_button": "ğŸ”“ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø±ÙˆØ¬",
        "no_comments_msg": "âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø±ÙŠÙ…Ø§ÙƒØªÙˆ Ù…ØªØ§Ø­Ø©.",
        "original_data_label": "ğŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© (Ø£ÙˆÙ„ 10 ØµÙÙˆÙ):",
        "norm_success": "âœ… ØªÙ… ØªØ·Ø¨ÙŠÙ‚ ØªÙ†Ù‚ÙŠØ­ Ø§Ù„Ù†Øµ.",
        "no_token_msg": "âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø±Ù…Ø² ÙÙŠ Ø§Ù„Ø¬Ù„Ø³Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¹ÙˆØ¯Ø© ÙˆØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„.",
        "token_invalid": "âŒ Ø§Ù„Ø±Ù…Ø² ØºÙŠØ± ØµØ§Ù„Ø­ Ø£Ùˆ Ù…Ù†ØªÙ‡ÙŠ. ÙŠØ±Ø¬Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù…Ø¬Ø¯Ø¯Ù‹Ø§.",
        "logged_in_as": "âœ… ØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¨Ø§Ø³Ù…:",
        "role_label": "(Ø§Ù„Ø¯ÙˆØ±: ",
        "closing_paren": ")",
        "projects_header": "ğŸ—ï¸ Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ø¨Ù„Ø¯ÙŠØ© (Ø±ÙŠÙ…Ø§ÙƒØªÙˆ)",
        "no_projects_msg": "âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø´Ø§Ø±ÙŠØ¹ Ø±ÙŠÙ…Ø§ÙƒØªÙˆ Ù…ØªØ§Ø­Ø©.",
        "projects_data_preview": "ğŸ“‚ Ø¹Ø±Ø¶ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹",
        "summaries_of_themes": "ğŸ“ ØªÙ„Ø®ÙŠØµ Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹",
        "proposals_feedback_tab": "ğŸ—³ï¸ Ø¥Ø¶Ø§ÙØ© Ø§Ù‚ØªØ±Ø§Ø­ Ø£Ùˆ Ù…Ù„Ø§Ø­Ø¸Ø§Øª",
        "extra_visualizations_tab": "ğŸ“ˆ ØªØµÙˆØ±Ø§Øª ÙˆØªØ­Ù„ÙŠÙ„Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©",
        "all_user_inputs_tab": "ğŸ—ƒï¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø© Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø·Ù†ÙŠÙ†"
    },
    "French": {
        "title": "ğŸ“Š Tableau de bord de l'utilisateur",
        "header_comments": "ğŸ’¬ Commentaires des citoyens (REMACTO)",
        "label_normalize": "ğŸ§¹ Appliquer une normalisation de l'arabe (optionnel)",
        "analysis_section": "ğŸ§  Analyse de sentiment et rÃ©sumÃ©s GPT + polaritÃ©",
        "proposal_header": "ğŸ“ Soumettre une nouvelle proposition ou un retour",
        "proposal_title_label": "ğŸ“Œ Titre de la proposition",
        "proposal_description_label": "ğŸ§¾ Description de la proposition",
        "proposal_submit_button": "ğŸ“¤ Soumettre la proposition",
        "feedback_label": "ğŸ’­ Vos commentaires",
        "feedback_button": "ğŸ“¬ Envoyer le commentaire",
        "logout_button": "ğŸ”“ Se dÃ©connecter",
        "no_comments_msg": "âš ï¸ Aucun commentaire REMACTO disponible.",
        "original_data_label": "ğŸ“‹ DonnÃ©es d'origine (10 premiÃ¨res lignes):",
        "norm_success": "âœ… Normalisation du texte appliquÃ©e.",
        "no_token_msg": "âš ï¸ Aucun jeton trouvÃ© dans la session. Veuillez vous reconnecter.",
        "token_invalid": "âŒ Jeton invalide ou expirÃ©. Veuillez vous reconnecter.",
        "logged_in_as": "âœ… ConnectÃ© en tant que:",
        "role_label": "(RÃ´le: ",
        "closing_paren": ")",
        "projects_header": "ğŸ—ï¸ Projets Municipaux (REMACTO)",
        "no_projects_msg": "âš ï¸ Aucun projet REMACTO disponible.",
        "projects_data_preview": "ğŸ“‚ AperÃ§u des donnÃ©es du projet",
        "summaries_of_themes": "ğŸ“ RÃ©sumÃ©s des thÃ¨mes du projet",
        "proposals_feedback_tab": "ğŸ—³ï¸ Propositions et retour",
        "extra_visualizations_tab": "ğŸ“ˆ Visualisations supplÃ©mentaires",
        "all_user_inputs_tab": "ğŸ—ƒï¸ Toutes les entrÃ©es des citoyens"
    },
    "Darija": {
        "title": "ğŸ“Š Ù„ÙˆØ­Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø¯Ø§Ø±Ø¬Ø©",
        "header_comments": "ğŸ’¬ ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ù†Ø§Ø³ (Ø±ÙŠÙ…Ø§ÙƒØªÙˆ)",
        "label_normalize": "ğŸ§¹ Ù†Ù‚Ù‘ÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø´ÙˆÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)",
        "analysis_section": "ğŸ§  ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù…Ø¹ GPT + Ø§Ù„Ø¨ÙˆÙ„Ø§Ø±ÙŠØªÙŠ",
        "proposal_header": "ğŸ“ Ø²ÙŠØ¯ Ø§Ù‚ØªØ±Ø§Ø­ Ø¬Ø¯ÙŠØ¯ ÙˆÙ„Ø§ Ø´ÙŠ Ù…Ù„Ø§Ø­Ø¸Ø©",
        "proposal_title_label": "ğŸ“Œ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­ Ø¨Ø§Ù„Ø¯Ø§Ø±Ø¬Ø©",
        "proposal_description_label": "ğŸ§¾ ÙˆØµÙ Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­ Ø¨Ø§Ù„ØªÙØ§ØµÙŠÙ„",
        "proposal_submit_button": "ğŸ“¤ ØµÙŠÙØ· Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­",
        "feedback_label": "ğŸ’­ ØªØ¹Ø·ÙŠÙ†Ø§ Ø±Ø£ÙŠÙƒ",
        "feedback_button": "ğŸ“¬ ØµÙŠÙØ· Ø±Ø£ÙŠÙƒ",
        "logout_button": "ğŸ”“ Ø®Ø±ÙˆØ¬",
        "no_comments_msg": "âš ï¸ Ù…Ø§ÙƒØ§ÙŠÙ†Ø§Ø´ ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø±ÙŠÙ…Ø§ÙƒØªÙˆ Ø¯Ø§Ø¨Ø§.",
        "original_data_label": "ğŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© (Ø£ÙˆÙ„ 10 ØµÙÙˆÙ):",
        "norm_success": "âœ… ØªØµØ§ÙˆØ§Ø¨Ø§Øª ØªÙ†Ù‚ÙŠØ© Ø§Ù„Ù†Øµ.",
        "no_token_msg": "âš ï¸ Ù…Ø§ÙƒÙŠÙ†Ø´ Ø§Ù„ØªÙˆÙƒÙ† ÙØ§Ù„Ø³ÙŠØ´Ù†. Ø±Ø¬Ø¹ Ø³ÙŠÙ†ÙŠ.",
        "token_invalid": "âŒ Ø§Ù„ØªÙˆÙƒÙ† Ø®Ø§ÙŠØ¨ ÙˆÙ„Ø§ Ø³Ø§Ù„Ø§. Ø®ØµÙƒ ØªØ³ÙŠÙ†ÙŠ.",
        "logged_in_as": "âœ… Ù†ØªØ§ Ø¯Ø§Ø®Ù„ Ø¨Ø§Ø³Ù…:",
        "role_label": "(Ø¯ÙˆØ±: ",
        "closing_paren": ")",
        "projects_header": "ğŸ—ï¸ Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ø¬Ù…Ø§Ø¹Ø© (Ø±ÙŠÙ…Ø§ÙƒØªÙˆ)",
        "no_projects_msg": "âš ï¸ Ù…Ø§ÙƒØ§ÙŠÙ† Ù„Ø§ Ù…Ø´Ø§Ø±ÙŠØ¹ Ù„Ø§ ÙˆØ§Ù„Ùˆ.",
        "projects_data_preview": "ğŸ“‚ Ø´ÙˆÙ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹",
        "summaries_of_themes": "ğŸ“ Ù„Ø®Øµ Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹",
        "proposals_feedback_tab": "ğŸ—³ï¸ Ø²ÙŠØ¯ Ø§Ù‚ØªØ±Ø§Ø­ ÙˆÙ„Ø§ Ù…Ù„Ø§Ø­Ø¸Ø©",
        "extra_visualizations_tab": "ğŸ“ˆ ØªØµØ§ÙˆØ± ÙˆØªØ­Ù„ÙŠÙ„Ø§Øª Ø²ÙˆÙŠÙ†Ø©",
        "all_user_inputs_tab": "ğŸ—ƒï¸ ÙƒÙ„Ø´ÙŠ Ø¯ÙŠØ§Ù„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¯ÙŠØ§Ù„ Ø§Ù„Ù†Ø§Ø³"
    }
}


##############################################################################
# 4) GPT Initialization + Language Dictionary
##############################################################################
def init_gpt():
    """
    Initialize OpenAI GPT with the key stored in st.secrets.
    """
    if not openai.api_key:
        openai.api_key = st.secrets["openai"]["api_key"]


##############################################################################
# 5) Utility: Normalizing Arabic, chunking, GPT for data
##############################################################################
def normalize_arabic(text: str) -> str:
    # (same as above)
    diacritics_pattern = re.compile(r'[\u0617-\u061A\u064B-\u0652]')
    text = re.sub(diacritics_pattern, '', text)
    text = re.sub(r'Ù€+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = ' '.join(text.split())
    return text.strip()



##############################################################################
# 3) GPT-based Helper Functions
##############################################################################
def gpt_arabic_sentiment_with_polarity(text: str) -> tuple:
    """
    Classify Arabic text with a sentiment label (POS/NEG/NEU)
    plus a numeric polarity from -1.0 to +1.0.
    Returns (sentiment_label, polarity_score).
    """
    text = text.strip()
    if not text:
        return ("NEU", 0.0)

    system_msg = "You are a helpful assistant for Arabic sentiment analysis."
    user_msg = f"""
    Ø­Ù„Ù„ Ø§Ù„Ø´Ø¹ÙˆØ± ÙÙŠ Ø§Ù„Ù†Øµ Ø£Ø¯Ù†Ø§Ù‡ ÙˆØ£Ø¹Ø·Ù Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§Ù‹ Ù…Ù† ÙØ¶Ù„Ùƒ:
    1) Ø§Ù„ØªØµÙ†ÙŠÙ: Ø§Ø®ØªØ± Ù…Ù† Ø¨ÙŠÙ† 'POS' Ø¥ÙŠØ¬Ø§Ø¨ÙŠØŒ 'NEG' Ø³Ù„Ø¨ÙŠØŒ Ø£Ùˆ 'NEU' Ù…Ø­Ø§ÙŠØ¯
    2) Ø¯Ø±Ø¬Ø© Ø±Ù‚Ù…ÙŠØ© Ø¨ÙŠÙ† -1.0 Ø¥Ù„Ù‰ +1.0

    Ø£Ø¬Ø¨ Ø¨ØµÙŠØºØ© JSON:
    {{
      "sentiment": "POS"/"NEG"/"NEU",
      "score": float
    }}

    Ø§Ù„Ù†Øµ:
    {text}
    """

    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
            max_tokens=100,
            temperature=0.0,
        )
        content = response["choices"][0]["message"]["content"].strip()

        parsed = {}
        try:
            parsed = json.loads(content)
        except:
            pass

        sentiment = parsed.get("sentiment", "NEU")
        score = float(parsed.get("score", 0.0))
        if sentiment not in ["POS", "NEG", "NEU"]:
            sentiment = "NEU"
        score = max(-1.0, min(1.0, score))
        return (sentiment, score)
    except Exception as e:
        st.warning(f"GPT Sentiment Error: {e}")
        return ("NEU", 0.0)


def gpt_bullet_summary(text: str) -> str:
    """
    Generate bullet-point summary in Arabic for the given text.
    """
    if not text.strip():
        return "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ Ù„Ù„Ø®Ù„Ø§ØµØ©."

    prompt = f"""
    Ù„Ø®Øµ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¹Ø¨Ø± Ù†Ù‚Ø§Ø· (bullet points):
    Ø§Ù„Ù†Øµ:
    {text}
    """
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system",
                    "content": "You are an assistant specialized in summarizing Arabic text into bullet points.",
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=250,
            temperature=0.0,
        )
        summary = response["choices"][0]["message"]["content"].strip()
        return summary
    except Exception as e:
        st.warning(f"GPT Bullet Summary Error: {e}")
        return "ØªØ¹Ø°Ù‘Ø± ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ù„Ø®Øµ."


def gpt_extract_pros_cons(text: str) -> dict:
    """
    Attempt to extract top pros and cons from a text using GPT.
    Returns {'pros': [...], 'cons': [...]}
    """
    if not text.strip():
        return {"pros": [], "cons": []}

    user_msg = f"""
    Ø§Ù‚Ø±Ø£ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ ÙˆØ§Ø³ØªØ®Ø±Ø¬ Ø£Ù‡Ù… Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© (Pros) ÙˆØ§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø³Ù„Ø¨ÙŠØ© (Cons):
    Ø§Ù„Ù†Øµ:
    {text}

    Ø§Ù„ØµÙŠØºØ©:
    Pros:
    - ...
    Cons:
    - ...
    """
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You extract pros and cons from Arabic text."},
                {"role": "user", "content": user_msg},
            ],
            max_tokens=300,
            temperature=0.0,
        )
        content = response["choices"][0]["message"]["content"].strip()

        pros = []
        cons = []
        lines = content.splitlines()
        current_section = None

        for line in lines:
            low_line = line.lower().strip()
            if low_line.startswith("pros"):
                current_section = "pros"
                continue
            elif low_line.startswith("cons"):
                current_section = "cons"
                continue
            elif line.strip().startswith("-"):
                if current_section == "pros":
                    pros.append(line.lstrip("-").strip())
                elif current_section == "cons":
                    cons.append(line.lstrip("-").strip())

        return {"pros": pros, "cons": cons}
    except Exception as e:
        st.warning(f"GPT Pros/Cons Error: {e}")
        return {"pros": [], "cons": []}


def gpt_extract_topics(text: str) -> list:
    """
    Use GPT to do basic "topic extraction" from Arabic text.
    Returns a list of discovered topics.
    """
    if not text.strip():
        return []

    user_msg = f"""
    Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø© ÙÙŠ Ø§Ù„Ù†Øµ:
    {text}
    """
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You extract key topics from Arabic text."},
                {"role": "user", "content": user_msg},
            ],
            max_tokens=150,
            temperature=0.0,
        )
        content = response["choices"][0]["message"]["content"].strip()
        topics = []
        for line in content.splitlines():
            line = line.strip("-â€¢123456789). ").strip()
            if line:
                topics.append(line)
        topics = list(dict.fromkeys(topics))
        return topics
    except Exception as e:
        st.warning(f"GPT Topic Modeling Error: {e}")
        return []


##############################################################################
# 4) Handling Large Arabic Text for GPT Translation
##############################################################################
def chunk_text(text: str, chunk_size: int = 2000) -> list:
    """
    Break a large string into smaller chunks (each up to chunk_size characters).
    This helps avoid large token usage errors in GPT.
    """
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start = end
    return chunks

def gpt_translate_arabic(text: str, target_language: str = "English") -> str:
    """
    Translate the given Arabic text to either English or French using GPT,
    chunking if necessary to avoid token limit errors.
    Also optionally limit total length or sample the text if data is extremely large.

    :param text: the Arabic source text
    :param target_language: "English" or "French"
    :return: translated text in the target language
    """

    text = text.strip()
    if not text:
        return ""

    # If text is extremely large, let's limit or sample:
    max_overall_length = 6000
    if len(text) > max_overall_length:
        lines = text.split('\n')
        random.shuffle(lines)
        lines = lines[:200]
        text = "\n".join(lines)[:max_overall_length]

    text_chunks = chunk_text(text, chunk_size=1500)
    translated_chunks = []

    # Prepare system and user prompts
    system_prompt = f"You translate Arabic text to {target_language}."
    for chunk in text_chunks:
        user_msg = f"""
Translate the following Arabic text into {target_language} without additional commentary:
{chunk}
"""
        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_msg},
                ],
                max_tokens=1000,
                temperature=0.0,
            )
            chunk_translation = response["choices"][0]["message"]["content"].strip()
            translated_chunks.append(chunk_translation)
        except Exception as e:
            st.warning(f"GPT Translate Error on chunk: {e}")
            continue

    return " ".join(translated_chunks)


##############################################################################
# 5) Load CSV Data
##############################################################################
def load_remacto_comments(csv_path: str) -> pd.DataFrame:
    """
    REMACTO Comments CSV:
    Ø±Ù‚Ù… Ø§Ù„ÙÙƒØ±Ø©,Ø§Ù„Ù‚Ù†Ø§Ø©,Ø§Ù„Ù…Ø­ÙˆØ±,Ù…Ø§ Ù‡ÙŠ Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª / Ø§Ù„Ø¥Ø´ÙƒØ§Ù„ÙŠØ§Øª Ø§Ù„Ù…Ø·Ø±ÙˆØ­Ø© ØŸ,Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø­Ù„ Ø§Ù„Ù…Ù‚ØªØ±Ø­ ØŸ
    """
    try:
        df = pd.read_csv(csv_path)
        df.columns = [
            "idea_id",
            "channel",
            "axis",
            "challenge",
            "proposed_solution",
        ]
        return df
    except Exception as e:
        st.error(f"Error loading REMACTO Comments CSV: {e}")
        return pd.DataFrame()

def load_remacto_projects(csv_path: str) -> pd.DataFrame:
    """
    REMACTO Projects CSV:
    titles,CT,CollectivitÃ© territorial,Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹
    """
    try:
        df = pd.read_csv(csv_path)
        df.columns = [
            "title",
            "CT",
            "collectivite_territoriale",
            "themes",
        ]
        return df
    except Exception as e:
        st.error(f"Error loading REMACTO Projects CSV: {e}")
        return pd.DataFrame()


##############################################################################
# 6) Wordcloud (with GPT-based translation to chosen language)
##############################################################################
def plot_wordcloud(texts: list, title: str, target_language: str = "English"):
    """
    1) Merge the list of Arabic texts into one string.
    2) Use GPT to translate that text to either English or French in manageable chunks.
    3) Generate a WordCloud from the translated text.
    """
    joined_text_ar = "\n".join(texts).strip()
    if not joined_text_ar:
        st.warning("No text found to generate wordcloud.")
        return

    with st.spinner(f"Translating text to {target_language} for WordCloud (may sample if data is huge)..."):
        translated_text = gpt_translate_arabic(joined_text_ar, target_language)

    if not translated_text.strip():
        st.warning("Translation returned empty. Cannot generate WordCloud.")
        return

    wc = WordCloud(
        width=800,
        height=400,
        background_color="white",
        collocations=False
    ).generate(translated_text)

    fig, ax = plt.subplots(figsize=(8, 4))
    ax.imshow(wc, interpolation="bilinear")
    ax.set_title(title, fontsize=16)
    ax.axis("off")
    st.pyplot(fig)


##############################################################################
# 7) Store Citizen Inputs
##############################################################################
def store_user_input_in_csv(username: str, input_type: str, content: str):
    """
    Append a row to 'user_inputs.csv' with columns:
      [timestamp, username, input_type, content]
    """
    timestamp = datetime.now().isoformat()
    row = {
        "timestamp": timestamp,
        "username": username,
        "input_type": input_type,
        "content": content,
    }
    csv_file = "user_inputs.csv"

    file_exists = os.path.exists(csv_file)
    df_new = pd.DataFrame([row])

    if not file_exists:
        df_new.to_csv(csv_file, index=False)
    else:
        df_existing = pd.read_csv(csv_file)
        df_combined = pd.concat([df_existing, df_new], ignore_index=True)
        df_combined.to_csv(csv_file, index=False)


##############################################################################
# 9) The Main Dashboard
##############################################################################
@require_auth
def main():
    # 1) Initialize GPT
    init_gpt()

    # 2) Determine the interface language from session_state
    #    (fallback to English if not present)
    lang = st.session_state.get("site_language", "English")
    if lang not in ui_texts:
        lang = "English"

    # Shortcut to the dictionary for the chosen language
    L = ui_texts[lang]

     # ======================== SIDEBAR (ARABIC) ========================
    st.sidebar.title("Ù„ÙˆØ­Ø© Ø¬Ø§Ù†Ø¨ÙŠØ© (Side Bar)")
    
    # A big welcome text for illiterate/literate
    st.sidebar.markdown("### ğŸ¤ Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨ÙƒÙ… ÙÙŠ Ø§Ù„Ù…Ù†ØµØ©!")
    st.sidebar.markdown("""
    Ù‡Ø°Ù‡ Ø§Ù„Ù„ÙˆØ­Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ© ØªÙ‚Ø¯Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø£Ø¯ÙˆØ§Øª:
    - Ø§Ù„Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ©
    - ØªÙƒØ¨ÙŠØ± Ø§Ù„Ø®Ø·
    - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£ÙŠÙ‚ÙˆÙ†Ø§Øª Ø§Ù„Ø¨ØµØ±ÙŠØ©
    """)
    
    # 1) Accessibility Expanders
    with st.sidebar.expander("ğŸ¦» Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ÙˆØµÙˆÙ„"):
        # Option: Increase font size
        font_size = st.selectbox("Ø­Ø¬Ù… Ø§Ù„Ø®Ø·:", ["ØµØºÙŠØ±", "Ù…ØªÙˆØ³Ø·", "ÙƒØ¨ÙŠØ±"])
        st.write("ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø®ØªÙŠØ§Ø± Ø­Ø¬Ù… Ø§Ù„Ø®Ø· Ø§Ù„Ù…Ù†Ø§Ø³Ø¨.")
        # Option: Provide a toggle for "High Contrast Mode"
        high_contrast = st.checkbox("ÙˆØ¶Ø¹ ØªØ¨Ø§ÙŠÙ† Ø¹Ø§Ù„Ù")
        st.write("Ù‡Ø°Ø§ Ø§Ù„ÙˆØ¶Ø¹ ÙŠØ±ÙØ¹ Ù…Ù† ÙˆØ¶ÙˆØ­ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ù„Ù„Ø£Ø´Ø®Ø§Øµ Ø°ÙˆÙŠ Ø§Ù„Ù‚Ø¯Ø±Ø© Ø§Ù„Ù…Ø­Ø¯ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¤ÙŠØ©.")
    
    # 2) Audio Guidance / TTS for illiterate users
    with st.sidebar.expander("ğŸ”Š Ù…Ø³Ø§Ø¹Ø¯Ø© ØµÙˆØªÙŠØ© (TTS)"):
        st.write("Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø§Ù„Ø°ÙŠÙ† Ù„Ø§ ÙŠÙ‚Ø±Ø¤ÙˆÙ† Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø³Ù‡ÙˆÙ„Ø©ØŒ ÙŠÙ…ÙƒÙ†Ù‡Ù… Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ø¥Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©.")
        # Placeholder: We can have a button to "Play Audio" or "Stop Audio"
        if st.button("ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„ØµÙˆØªÙŠØ©"):
            st.info("ğŸš§ ÙŠØ¬Ø±ÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„ØµÙˆØªÙŠØ©... (Ù†Ù…ÙˆØ°Ø¬ ØªØ¬Ø±ÙŠØ¨ÙŠ)")
        if st.button("Ø¥ÙŠÙ‚Ø§Ù"):
            st.info("ğŸš§ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„ØµÙˆØªÙŠØ©.")
    
    # 3) Icons / Visual Aids
    st.sidebar.markdown("### ğŸ·ï¸ Ø±Ù…ÙˆØ² Ø¨ØµØ±ÙŠØ© Ù…Ø³Ø§Ø¹Ø¯Ø©:")
    st.sidebar.write("ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ù„Ø§Ø­Ø¸Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£ÙŠÙ‚ÙˆÙ†Ø§Øª Ù„ØªØ³Ù‡ÙŠÙ„ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ø³Ø§Ù…:")
    st.sidebar.write("- ğŸ“Š Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©")
    st.sidebar.write("- ğŸ“ Ù„Ù„Ø¥Ù‚ØªØ±Ø§Ø­Ø§Øª")
    st.sidebar.write("- ğŸ’­ Ù„Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª")
    st.sidebar.write("- ğŸ”“ Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø±ÙˆØ¬")
    
    # 4) Possibly a "Language Switcher" in Arabic (though we have a site_language from login)
    with st.sidebar.expander("ğŸŒ ØªØºÙŠÙŠØ± Ø§Ù„Ù„ØºØ©"):
        chosen_lang = st.selectbox("Ø§Ø®ØªØ± Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¶", ["Arabic", "English", "French", "Darija"], index=0)
        if st.button("ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù„ØºØ©"):
            st.session_state.site_language = chosen_lang
            st.experimental_rerun()

    # Title & Description
    st.title(L["title"])
    st.write("Welcome to your personal dashboard! Engage with projects, share feedback, see analytics, etc.")

    # Check JWT token
    token = st.session_state.get("jwt_token", None)
    if token:
        is_valid, username, role = verify_jwt_token(token)
        if is_valid:
            # Show a success message with user's role
            st.success(f"{L['logged_in_as']} **{username}** {L['role_label']}{role}{L['closing_paren']}")

            # Logout button
            if st.button(L["logout_button"]):
                st.session_state.jwt_token = None
                st.experimental_rerun()

            # CSV paths (adjust as needed)
            comments_csv_path = "REMACTO Comments.csv"
            projects_csv_path = "REMACTO Projects.csv"

            df_comments = load_remacto_comments(comments_csv_path)
            df_projects = load_remacto_projects(projects_csv_path)

            # Create main tabs
            tabs = st.tabs(
                [
                    L["header_comments"],             # e.g. "Citizen Comments (REMACTO)"
                    L["projects_header"],             # e.g. "Municipal Projects (REMACTO)"
                    L["proposals_feedback_tab"],       # e.g. "Submit a New Proposal or Feedback"
                    L["extra_visualizations_tab"],     # e.g. "Extra Visualizations & Analysis"
                    L["all_user_inputs_tab"],          # e.g. "All Stored Inputs from Citizens"
                ]
            )

            # -----------------------------------------------------------------
            # TAB 1: Comments Analysis
            # -----------------------------------------------------------------
            with tabs[0]:
                st.header(L["header_comments"])
                if df_comments.empty:
                    st.warning(L["no_comments_msg"])
                else:
                    st.write(f"### {L['original_data_label']}")
                    st.dataframe(df_comments.head(10))

                    st.write(f"#### {L['label_normalize']}")
                    do_normalize = st.checkbox("Normalize Text?")
                    df_comments_proc = df_comments.copy()

                    if do_normalize:
                        df_comments_proc["challenge"] = df_comments_proc["challenge"].apply(normalize_arabic)
                        df_comments_proc["proposed_solution"] = df_comments_proc["proposed_solution"].apply(normalize_arabic)
                        st.success(L["norm_success"])

                    # Filter by axis
                    unique_axes = df_comments_proc["axis"].unique()
                    selected_axis = st.selectbox("Filter by Axis:", ["All"] + list(unique_axes))
                    if selected_axis != "All":
                        filtered_comments = df_comments_proc[df_comments_proc["axis"] == selected_axis]
                    else:
                        filtered_comments = df_comments_proc

                    st.write(f"Total {len(filtered_comments)} comments after axis filter: {selected_axis}")

                    # GPT-based Analysis
                    st.write(f"### {L['analysis_section']}")
                    num_rows = st.slider("Number of Rows to Analyze", 1, min(50, len(filtered_comments)), 5)

                    analysis_data = []
                    with st.spinner("Analyzing with GPT..."):
                        for i in range(num_rows):
                            row = filtered_comments.iloc[i]
                            challenge_text = row["challenge"]
                            solution_text = row["proposed_solution"]

                            # 1) Sentiment + Polarity
                            sentiment, polarity_score = gpt_arabic_sentiment_with_polarity(challenge_text)

                            # 2) Bullet Summary
                            bullet_challenge = gpt_bullet_summary(challenge_text)

                            # 3) Pros & Cons
                            pros_cons = gpt_extract_pros_cons(solution_text)
                            pros_join = "; ".join(pros_cons["pros"]) if pros_cons["pros"] else ""
                            cons_join = "; ".join(pros_cons["cons"]) if pros_cons["cons"] else ""

                            # 4) Topics
                            topics = gpt_extract_topics(challenge_text)
                            topics_join = "; ".join(topics)

                            analysis_data.append({
                                "idea_id": row["idea_id"],
                                "axis": row["axis"],
                                "channel": row["channel"],
                                "challenge_sentiment": sentiment,
                                "polarity_score": polarity_score,
                                "challenge_summary_bullets": bullet_challenge,
                                "solution_pros": pros_join,
                                "solution_cons": cons_join,
                                "extracted_topics": topics_join,
                            })

                    df_analysis = pd.DataFrame(analysis_data)
                    st.dataframe(df_analysis)

                    # Polarity distribution
                    st.write("#### Polarity Distribution (Histogram)")
                    fig_pol, ax_pol = plt.subplots()
                    ax_pol.hist(df_analysis["polarity_score"], bins=10, color="skyblue")
                    ax_pol.set_title("Polarity Score Distribution")
                    ax_pol.set_xlabel("Score (-1 = negative, +1 = positive)")
                    ax_pol.set_ylabel("Count")
                    st.pyplot(fig_pol)

                    # Pie chart of sentiment distribution
                    sentiment_counts = df_analysis["challenge_sentiment"].value_counts()
                    st.write("#### Sentiment Distribution")
                    fig_sent, ax_sent = plt.subplots()
                    ax_sent.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct="%1.1f%%")
                    ax_sent.axis("equal")
                    st.pyplot(fig_sent)

                    # Wordcloud of challenges, translated to user-chosen language for display
                    st.write(f"#### Word Cloud (Challenges) in {lang}")
                    plot_wordcloud(
                        filtered_comments["challenge"].astype(str).tolist(),
                        f"Challenges Word Cloud ({lang})",
                        target_language="English" if lang in ["English", "Darija"] else lang
                    )

            # -----------------------------------------------------------------
            # TAB 2: Projects
            # -----------------------------------------------------------------
            with tabs[1]:
                st.header(L["projects_header"])
                if df_projects.empty:
                    st.warning(L["no_projects_msg"])
                else:
                    st.write(f"### {L['projects_data_preview']}")
                    st.dataframe(df_projects.head(10))

                    st.write(f"### {L['summaries_of_themes']}")
                    max_rows_proj = st.slider("Number of Projects to Summarize", 1, len(df_projects), 5)
                    project_summaries = []
                    with st.spinner("Summarizing project themes..."):
                        for idx in range(max_rows_proj):
                            row = df_projects.iloc[idx]
                            theme_text = row["themes"]
                            bullet_sum = gpt_bullet_summary(theme_text)
                            project_summaries.append({
                                "title": row["title"],
                                "themes": theme_text,
                                "bullet_summary": bullet_sum,
                            })

                    st.write(pd.DataFrame(project_summaries))

                    # Quick bar chart
                    st.write("### Projects by CT")
                    ct_counts = df_projects["CT"].value_counts()
                    st.bar_chart(ct_counts)

            # -----------------------------------------------------------------
            # TAB 3: Proposals & Feedback
            # -----------------------------------------------------------------
            with tabs[2]:
                st.header(L["proposal_header"])

                st.subheader(L["proposal_title_label"])
                proposal_title = st.text_input(L["proposal_title_label"], placeholder="e.g. Ù…Ø³Ø§Ø±Ø§Øª Ø®Ø§ØµØ© Ù„Ù„Ø¯Ø±Ø§Ø¬Ø§Øª ÙÙŠ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©")

                st.subheader(L["proposal_description_label"])
                proposal_description = st.text_area(L["proposal_description_label"], placeholder="Describe your idea in detail...")

                if st.button(L["proposal_submit_button"]):
                    if proposal_title.strip() and proposal_description.strip():
                        content = f"Title: {proposal_title}\nDescription: {proposal_description}"
                        store_user_input_in_csv(username, "proposal", content)
                        st.success("Proposal submitted successfully!")
                    else:
                        st.warning("Please provide both title and description.")

                st.subheader(L["feedback_label"])
                feedback_text = st.text_area(L["feedback_label"], placeholder="Any feedback or concerns about the city projects?")
                if st.button(L["feedback_button"]):
                    if feedback_text.strip():
                        store_user_input_in_csv(username, "feedback", feedback_text)
                        st.success("Your feedback has been recorded.")
                    else:
                        st.warning("Please enter some feedback.")

            # -----------------------------------------------------------------
            # TAB 4: Extra Visualizations
            # -----------------------------------------------------------------
            with tabs[3]:
                st.header(L["extra_visualizations_tab"])

                if df_comments.empty:
                    st.info(L["no_comments_msg"])
                else:
                    # Axis distribution
                    axis_counts = df_comments["axis"].value_counts()
                    st.write("### Axis Distribution (Bar Chart)")
                    st.bar_chart(axis_counts)

                    # Channel distribution
                    channel_counts = df_comments["channel"].value_counts()
                    st.write("### Channels (Pie Chart)")
                    fig_c, ax_c = plt.subplots()
                    ax_c.pie(channel_counts.values, labels=channel_counts.index, autopct="%1.1f%%")
                    ax_c.axis("equal")
                    st.pyplot(fig_c)

                    st.write(f"### Word Cloud of Proposed Solutions (in {lang})")
                    plot_wordcloud(
                        df_comments["proposed_solution"].astype(str).tolist(),
                        f"Proposed Solutions ({lang})",
                        target_language="English" if lang in ["English", "Darija"] else lang
                    )

            # -----------------------------------------------------------------
            # TAB 5: All User Inputs
            # -----------------------------------------------------------------
            with tabs[4]:
                st.header(L["all_user_inputs_tab"])
                csv_file = "user_inputs.csv"
                if not os.path.exists(csv_file):
                    st.info("No user inputs stored yet.")
                else:
                    df_user_inputs = pd.read_csv(csv_file)
                    st.dataframe(df_user_inputs)

                    if role != "admin":
                        df_user_specific = df_user_inputs[df_user_inputs["username"] == username]
                        st.write(f"Showing inputs for user: **{username}**")
                        st.dataframe(df_user_specific)

                    st.write("### Export Citizen Inputs as CSV")
                    csv_data = df_user_inputs.to_csv(index=False).encode("utf-8")
                    st.download_button(
                        label="Download CSV",
                        data=csv_data,
                        file_name="user_inputs_all.csv",
                        mime="text/csv"
                    )

        else:
            st.warning(L["token_invalid"])
    else:
        st.info(L["no_token_msg"])


# If running standalone
if __name__ == "__main__":
    main()